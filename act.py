"""
===============================================================
Generic Summary Evaluator (Expected vs Generated)
===============================================================

ğŸ§© Purpose:
-----------
Compare the generated summary against the expected (ground truth)
and evaluate how accurate and contextually consistent it is.

âœ¨ Evaluation Dimensions:
-------------------------
1. Numeric Accuracy  â†’  Are numbers (totals, percentages, etc.) correct?
2. Context Similarity â†’  Does the meaning and intent align semantically?
3. Final Score        â†’  Weighted blend of both metrics.

âœ… Outputs:
-----------
- numeric_score (0 to 1)
- context_score (0 to 1)
- final_score   (0 to 1)
- verdict       (Excellent / Good / Fair / Poor / Failed)
- explanation   (Human-readable reason)

---------------------------------------------------------------
Author : Rahul Chavan (Data Engineer)
Evaluator: GPT-5 (Langfuse Integrated)
---------------------------------------------------------------
"""

import re
from langfuse import Langfuse

# Initialize Langfuse client
lf = Langfuse()

# -------------------------
# 1ï¸âƒ£ Helper: Extract numbers
# -------------------------
def extract_numbers(text: str):
    """
    Extracts numeric values (int/float) from any text.
    Example:
      "Total sales = 1200.5 units" â†’ [1200.5]
    """
    return [float(x) for x in re.findall(r"\d+(?:\.\d+)?", text)]


# -------------------------
# 2ï¸âƒ£ Numeric Score
# -------------------------
def numeric_score(expected: str, generated: str) -> float:
    """
    Compares numeric values from expected vs generated summaries.
    Uses relative difference to score proximity (0â€“1).
    """
    exp_nums = extract_numbers(expected)
    gen_nums = extract_numbers(generated)
    if not exp_nums or not gen_nums:
        return 1.0  # no numbers to compare â†’ assume perfect

    scores = []
    for e, g in zip(exp_nums, gen_nums):
        diff_ratio = abs(e - g) / max(e, 1)
        score = max(0, 1 - diff_ratio)
        scores.append(score)

    return sum(scores) / len(scores)


# -------------------------
# 3ï¸âƒ£ Contextual Score (LLM Judge)
# -------------------------
def context_score(expected: str, generated: str) -> float:
    """
    Uses Langfuse LLM-as-a-Judge to assess semantic alignment.
    Returns a similarity score between 0 and 1.
    """
    result = lf.evaluation.run(
        name="context_eval",
        input={"expected": expected, "generated": generated},
        model="gpt-4o-mini",
        instructions="""
        Evaluate how semantically aligned the two summaries are.
        Return a single numeric score between 0 and 1:
        - 1 â†’ identical meaning
        - 0.5 â†’ somewhat related
        - 0 â†’ completely different
        Output only the float number.
        """
    )

    try:
        return float(result.output_text.strip())
    except:
        return 0.0


# -------------------------
# 4ï¸âƒ£ Verdict & Explanation
# -------------------------
def verdict_label(score: float) -> str:
    """
    Maps numeric score to human-readable verdict label.
    """
    if score >= 0.95: return "Excellent âœ…"
    elif score >= 0.9: return "Good ğŸ‘"
    elif score >= 0.8: return "Fair âš ï¸"
    elif score >= 0.6: return "Poor âŒ"
    else: return "Failed ğŸš«"


def explanation_comment(num_score: float, ctx_score: float) -> str:
    """
    Generates short evaluation explanation for logs or dashboards.
    """
    if num_score < 0.8 and ctx_score < 0.8:
        return "Both numeric values and context differ significantly."
    elif num_score < 0.8:
        return "Numeric mismatch detected, context mostly aligned."
    elif ctx_score < 0.8:
        return "Context drift detected despite numeric correctness."
    elif num_score >= 0.95 and ctx_score >= 0.95:
        return "Perfect match in both numeric and contextual meaning."
    else:
        return "Minor variations but overall consistent summary."


# -------------------------
# 5ï¸âƒ£ Master Evaluation Function
# -------------------------
def evaluate_summary(expected: str, generated: str):
    """
    Runs numeric + context evaluation and computes final score.
    Returns JSON-like dictionary for direct use in dashboards or APIs.
    """
    num = numeric_score(expected, generated)
    ctx = context_score(expected, generated)

    # Equal weight blend (can be tuned)
    final = round((0.5 * num) + (0.5 * ctx), 3)

    return {
        "numeric_score": round(num, 3),
        "context_score": round(ctx, 3),
        "final_score": final,
        "verdict": verdict_label(final),
        "explanation": explanation_comment(num, ctx)
    }


# -------------------------
# 6ï¸âƒ£ Example Run
# -------------------------
if __name__ == "__main__":
    expected = "Total sales were 1200 till March 2024 with a 10% growth rate."
    generated = "Sales reached 1180 till March 2024 showing 10 percent growth."

    result = evaluate_summary(expected, generated)
    print(result)


#output
{
  "numeric_score": 0.983,
  "context_score": 0.95,
  "final_score": 0.967,
  "verdict": "Excellent âœ…",
  "explanation": "Minor variations but overall consistent summary."
}

#ğŸ§  How It Works (Quick Explanation)
Step	What Happens	Example
1. Number extraction	Pulls all numeric values using regex	â€œ1200â€, â€œ10â€
2. Numeric score	Calculates how close generated numbers are to expected	1180 â‰ˆ 1200 â†’ 0.983
3. Context score	Uses LLM (via Langfuse) to check semantic match	â€œgrowth rateâ€ â‰ˆ â€œpercent growthâ€ â†’ 0.95
4. Verdict	Maps final score to qualitative label	0.967 â†’ â€œExcellent âœ…â€
5. Explanation	Adds a short reasoning summary	â€œMinor variations but overall consistent summary.â€
âš¡ Advantages

âœ… Works for any kind of summary â€” text, table, or report
âœ… Balanced numeric + meaning-based scoring
âœ… LLM-powered semantic judgment
âœ… Human-readable verdict & explanation for dashboards
âœ… Plug-and-play with Langfuseâ€™s evaluation tracking
